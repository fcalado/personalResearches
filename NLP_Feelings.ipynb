{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install wordcloud\n",
    "!pip install nltk\n",
    "!pip install seaborn\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import unidecode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import wordcloud\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"all\")\n",
    "# nltk.download(\"ptb\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"data/imdb-reviews-pt-br.csv\")\n",
    "reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.model_selection import train_test_split\n",
    "# train, test, class_train, class_test = train_test_split(reviews.text_pt, reviews.sentiment, random_state = 42)\n",
    "\n",
    "# # from sklearn.linear_model import LogisticRegression\n",
    "# RegressionLogistic = LogisticRegression()\n",
    "# RegressionLogistic.fit(train, class_train)\n",
    "# acuracy = RegressionLogistic.score(test, class_test)\n",
    "# print(acuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = reviews['sentiment'].replace(['neg','pos'],[0,1])\n",
    "reviews['classification'] = classification\n",
    "reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['Assisti um filme maravilhoso','Assisti um filme muito bom','Filme p√©ssimo']\n",
    "vetorize = CountVectorizer(lowercase=False)\n",
    "bag_of_words = vetorize.fit_transform(text)\n",
    "vetorize.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sparce = pd.DataFrame.sparse.from_spmatrix(bag_of_words, columns=vetorize.get_feature_names_out())\n",
    "matrix_sparce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vetorize = CountVectorizer(lowercase=False)\n",
    "#vetorize = CountVectorizer()\n",
    "# vetorize = CountVectorizer(max_features=100)\n",
    "# bag_of_words = vetorize.fit_transform(reviews.text_pt)\n",
    "# bag_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetorize = CountVectorizer(max_features=100)\n",
    "bag_of_words = vetorize.fit_transform(reviews.text_pt)\n",
    "bag_of_words.shape\n",
    "train, test, class_train, class_test = train_test_split(bag_of_words, reviews.classification, random_state = 42)\n",
    "RegressionLogistic = LogisticRegression(max_iter=100)\n",
    "RegressionLogistic.fit(train, class_train)\n",
    "acuracy = RegressionLogistic.score(test, class_test)\n",
    "print(acuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetorize = CountVectorizer(max_features=300)\n",
    "bag_of_words = vetorize.fit_transform(reviews.text_pt)\n",
    "bag_of_words.shape\n",
    "train, test, class_train, class_test = train_test_split(bag_of_words, reviews.classification, random_state = 42)\n",
    "RegressionLogistic = LogisticRegression(max_iter=300)\n",
    "RegressionLogistic.fit(train, class_train)\n",
    "acuracy = RegressionLogistic.score(test, class_test)\n",
    "print(acuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetorize = CountVectorizer(max_features=500)\n",
    "bag_of_words = vetorize.fit_transform(reviews.text_pt)\n",
    "bag_of_words.shape\n",
    "train, test, class_train, class_test = train_test_split(bag_of_words, reviews.classification, random_state = 42)\n",
    "RegressionLogistic = LogisticRegression(max_iter=500)\n",
    "RegressionLogistic.fit(train, class_train)\n",
    "acuracy = RegressionLogistic.score(test, class_test)\n",
    "print(acuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetorize = CountVectorizer(max_features=1000)\n",
    "bag_of_words = vetorize.fit_transform(reviews.text_pt)\n",
    "bag_of_words.shape\n",
    "train, test, class_train, class_test = train_test_split(bag_of_words, reviews.classification, random_state = 42)\n",
    "# RegressionLogistic = LogisticRegression(max_iter=500)\n",
    "RegressionLogistic = LogisticRegression(max_iter=1000,solver='saga')\n",
    "RegressionLogistic.fit(train, class_train)\n",
    "acuracy = RegressionLogistic.score(test, class_test)\n",
    "print(acuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine all reviews into one string\n",
    "all_words = ' '.join([text for text in reviews.text_pt])\n",
    "\n",
    "# Generate the word cloud\n",
    "word_cloud = WordCloud(width=800, height=500, max_font_size=110, collocations=False).generate(all_words)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(all_words)\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloudWordsNegative(text, column_text):\n",
    "    text_negative = text.query(\"sentiment == 'neg'\")\n",
    "    # Combine all reviews into one string\n",
    "    all_words = ' '.join([text for text in text_negative[column_text]])\n",
    "\n",
    "    # Generate the word cloud\n",
    "    word_cloud = WordCloud(width=800, height=500, max_font_size=110, collocations=False).generate(all_words)\n",
    "\n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.imshow(word_cloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def cloudWordsPositive(text, column_text):\n",
    "    text_positive = text.query(\"sentiment == 'pos'\")\n",
    "    # Combine all reviews into one string\n",
    "    all_words = ' '.join([text for text in text_positive[column_text]])\n",
    "\n",
    "    # Generate the word cloud\n",
    "    word_cloud = WordCloud(width=800, height=500, max_font_size=110, collocations=False).generate(all_words)\n",
    "\n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.imshow(word_cloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudWordsNegative(reviews,\"text_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudWordsPositive(reviews,\"text_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_space = tokenize.WhitespaceTokenizer()\n",
    "token_reviews = token_space.tokenize(all_words)\n",
    "frequency = nltk.FreqDist(token_reviews)\n",
    "df_frequency = pd.DataFrame({\n",
    "                                'Word': list(frequency.keys()),\n",
    "                                'Frequency': list(frequency.values())\n",
    "                            })\n",
    "df_frequency.nlargest(columns= \"Frequency\", n = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def graphBar(df_frequency):\n",
    "def graphBar(df, df_column, quantity):\n",
    "    all_words = ' '.join([df for df in df[df_column]])\n",
    "    token_space = tokenize.WhitespaceTokenizer()\n",
    "    token_reviews = token_space.tokenize(all_words)\n",
    "    frequency = nltk.FreqDist(token_reviews)\n",
    "    df_frequency = pd.DataFrame({\n",
    "                                    'Word': list(frequency.keys()),\n",
    "                                    'Frequency': list(frequency.values())\n",
    "                                })\n",
    "    df2 = df_frequency.nlargest(columns= \"Frequency\", n = quantity)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = sns.barplot(data = df2, x= \"Word\", y = \"Frequency\", color = \"gray\")\n",
    "    ax.set(ylabel = \"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphBar(reviews, \"text_pt\", 20)\n",
    "# graphBar(df_frequency.nlargest(columns= \"Frequency\", n = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_word = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "print(irrelevant_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews.drop(columns=['tratamento_1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def removeStopWords(df):\n",
    "#     irrelevant_word = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "#     line_string = list()\n",
    "#     for review in df.text_pt:\n",
    "#         new_line = list()\n",
    "#         token_space = tokenize.WhitespaceTokenizer()\n",
    "#         new_line = token_space.tokenize(review)\n",
    "#         for word in new_line:\n",
    "#             if word not in irrelevant_word:\n",
    "#                 new_line.append(word)\n",
    "#         line_string.append(' '.join(new_line))\n",
    "#     df['tratamento_1'] = line_string\n",
    "\n",
    "# removeStopWords(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Version\n",
    "def removeStopWords(df):\n",
    "    # Load stopwords\n",
    "    irrelevant_words = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    # Create a list to store the processed reviews\n",
    "    line_string = []\n",
    "    # Iterate through each review\n",
    "    for review in df['text_pt']:\n",
    "        # Tokenize the review into words\n",
    "        words_line = tokenizer.tokenize(review)\n",
    "        # Remove stopwords\n",
    "        new_line = [word for word in words_line if word not in irrelevant_words]\n",
    "        # Join the words back into a string and add to the list\n",
    "        line_string.append(' '.join(new_line))\n",
    "    # Add the processed reviews as a new column\n",
    "    df['treatment'] = line_string\n",
    "\n",
    "removeStopWords(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(df, df_column, df_column_classif, iteration):\n",
    "    vetorize = CountVectorizer(max_features=iteration)\n",
    "    bag_of_words = vetorize.fit_transform(df[df_column])\n",
    "    bag_of_words.shape\n",
    "    train, test, class_train, class_test = train_test_split(bag_of_words, df[df_column_classif], random_state = 42)\n",
    "    # RegressionLogistic = LogisticRegression(max_iter=500)\n",
    "    RegressionLogistic = LogisticRegression(max_iter=iteration,solver='saga')\n",
    "    RegressionLogistic.fit(train, class_train)\n",
    "    acuracy = RegressionLogistic.score(test, class_test)\n",
    "    print(acuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification(reviews, \"treatment\",\"classification\",500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphBar(reviews,\"treatment\",20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# frase = \"Os cursos da Alura s√£o √≥timos, al√©m de √≥timos, t√™m alunos √≥timos!\"\n",
    "# token_espaco = WhitespaceTokenizer()\n",
    "# token_pontuacao = WordPunctTokenizer()\n",
    "\n",
    "# token_1 = token_espaco.tokenize(frase)\n",
    "# token_2 = token_pontuacao.tokenize(frase)\n",
    "# print(token_1)\n",
    "# print(token_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatmentsToClean(df, df_column):\n",
    "    token_punctuation = WordPunctTokenizer()\n",
    "    stemmer = nltk.RSLPStemmer()\n",
    "\n",
    "    # Aggregate a DataFrame List of Punctuation to remove\n",
    "    characters_and_words_to_remove = []\n",
    "    for punct in punctuation:\n",
    "        characters_and_words_to_remove.append(punct)\n",
    "\n",
    "    # Aggregate a DataFrame List of Punctuation + Irrelevant Words to remove\n",
    "    characters_and_words_to_remove = characters_and_words_to_remove + irrelevant_word\n",
    "\n",
    "    # Aggregate a DataFrame List of Punctuation + Irrelevant Words to remove + Words without Accents\n",
    "    characters_and_words_to_remove = [unidecode.unidecode(review) for review in characters_and_words_to_remove]\n",
    "\n",
    "    new_review_ok = []\n",
    "    for review in df[df_column]:\n",
    "        new_review = []\n",
    "        review = review.lower()\n",
    "        words_review = token_punctuation.tokenize(review)\n",
    "        for word in words_review:\n",
    "            if word not in characters_and_words_to_remove:\n",
    "                # Alter eath word for their prefix with the \"nltk.RSLPStemmer.stem(word)\"\n",
    "                new_review.append(stemmer.stem(word))\n",
    "        new_review_ok.append(' '.join(new_review))\n",
    "\n",
    "    df[df_column] = new_review_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatmentsToClean(reviews,'treatment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification(reviews, \"text_pt\",\"classification\",500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification(reviews, \"treatment\",\"classification\",500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = nltk.RSLPStemmer()\n",
    "# a = stemmer.stem(\"corredor\")\n",
    "# b = stemmer.stem(\"corre\")\n",
    "# c = stemmer.stem(\"correria\")\n",
    "# d = stemmer.stem(\"correlacionado\")\n",
    "# e = stemmer.stem(\"corrim√£o\")\n",
    "# print(a,b,c,d,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Assisti um √≥timo filme','Assisti um filme p√©ssimo']\n",
    "tfidf = TfidfVectorizer(max_features=500)\n",
    "specs = tfidf.fit_transform(sentences)\n",
    "pd.DataFrame(\n",
    "        specs.todense(),\n",
    "        columns=tfidf.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_original = tfidf.fit_transform(reviews['text_pt'])\n",
    "train, test, class_train, class_test = train_test_split(tfidf_original, reviews['classification'], random_state=42)\n",
    "\n",
    "RegressionLogistic.fit(train, class_train)\n",
    "acuracy_tfidf_original = RegressionLogistic.score(test, class_test)\n",
    "print(acuracy_tfidf_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_treated = tfidf.fit_transform(reviews['treatment'])\n",
    "train, test, class_train, class_test = train_test_split(tfidf_treated, reviews['classification'], random_state=42)\n",
    "\n",
    "RegressionLogistic.fit(train, class_train)\n",
    "acuracy_tfidf_treated = RegressionLogistic.score(test, class_test)\n",
    "print(acuracy_tfidf_treated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "vector_tfidf = tfidf.fit_transform(reviews['treatment'])\n",
    "train, test, class_train, class_test = train_test_split(vector_tfidf, reviews['classification'],random_state=42)\n",
    "RegressionLogistic.fit(train, class_train)\n",
    "acuracy_tfidf_treated_ngrams = RegressionLogistic.score(test, class_test)\n",
    "print(acuracy_tfidf_treated_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "vector_tfidf = tfidf.fit_transform(reviews['treatment'])\n",
    "train, test, class_train, class_test = train_test_split(vector_tfidf, reviews['classification'],random_state=42)\n",
    "RegressionLogistic.fit(train, class_train)\n",
    "acuracy_tfidf_treated_no_ngrams = RegressionLogistic.score(test, class_test)\n",
    "print(acuracy_tfidf_treated_no_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_pos = pd.DataFrame(\n",
    "    RegressionLogistic.coef_[0].T,\n",
    "    index= tfidf.get_feature_names_out()\n",
    ")\n",
    "weight_pos.nlargest(10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_neg = pd.DataFrame(\n",
    "    RegressionLogistic.coef_[0].T,\n",
    "    index= tfidf.get_feature_names_out()\n",
    ")\n",
    "weight_neg.nsmallest(10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
